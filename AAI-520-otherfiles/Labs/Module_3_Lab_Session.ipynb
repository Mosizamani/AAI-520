{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aUhhDt5jf9vJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.35.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/mostafazamaniturk/.venvs/py313/lib/python3.13/site-packages (from transformers) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/mostafazamaniturk/.venvs/py313/lib/python3.13/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/mostafazamaniturk/.venvs/py313/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/mostafazamaniturk/.venvs/py313/lib/python3.13/site-packages (from transformers) (2025.9.1)\n",
            "Requirement already satisfied: requests in /Users/mostafazamaniturk/.venvs/py313/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/mostafazamaniturk/.venvs/py313/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mostafazamaniturk/.venvs/py313/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/mostafazamaniturk/.venvs/py313/lib/python3.13/site-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/mostafazamaniturk/.venvs/py313/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mostafazamaniturk/.venvs/py313/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/mostafazamaniturk/.venvs/py313/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
            "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.35.0-py3-none-any.whl (563 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.4/563.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
            "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: safetensors, hf-xet, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [transformers][0m [transformers]ub]\n",
            "\u001b[1A\u001b[2KSuccessfully installed filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.0 safetensors-0.6.2 tokenizers-0.22.0 transformers-4.56.1\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXJgLRDHkdam",
        "outputId": "5f9a9f15-c263-4b98-dbce-dc891ecf3b4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: I really enjoyed the movie. The acting was fantastic!\n",
            "Predicted Label: Positive\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)  # Binary classification\n",
        "\n",
        "# Define a text for classification\n",
        "text = \"I really enjoyed the movie. The acting was fantastic!\"\n",
        "\n",
        "# Tokenize the text and convert to input features\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask).logits\n",
        "\n",
        "# Get predicted label\n",
        "predicted_label = torch.argmax(outputs, dim=1).item()\n",
        "\n",
        "# Define label names\n",
        "label_names = [\"Negative\", \"Positive\"]\n",
        "\n",
        "# Print result\n",
        "print(\"Text:\", text)\n",
        "print(\"Predicted Label:\", label_names[predicted_label])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_GWcUDTDvlHc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10",
      "language": "python",
      "name": "py311"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
