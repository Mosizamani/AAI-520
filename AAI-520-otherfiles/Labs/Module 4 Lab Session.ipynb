{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8b29ed-9096-4db1-9be3-826a5daad55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai langchain faiss-cpu sentence-transformers\n",
    "#! pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4de108-400d-4667-bf05-a209082da613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Required Libraries for LLM + Document Retrieval Workflow\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a637f-97ac-43df-b5ba-2aa19c56078b",
   "metadata": {},
   "source": [
    "LangChain is a framework for building applications with language models (LLMs). \n",
    "- It simplifies the process of integrating LLMs with external data sources (like documents, databases, or APIs) \n",
    "- It allows you to create intelligent apps such as chatbots, retrieval-augmented generation (RAG) systems, and more.\n",
    "\n",
    "LangChain acts as the \"glue\" that connects:\n",
    "\n",
    "* Your documents (TextLoader)---------------------------------------> Loads plain text documents into LangChain.\n",
    "* Your text processing (TextSplitter)-------------------------------> Splits long text into manageable chunks.\n",
    "* Your embeddings (HuggingFaceEmbeddings)---------------------------> Generates embeddings for document chunks.\n",
    "* Your retrieval system (FAISS)-------------------------------------> Stores and retrieves embeddings efficiently using similarity    search.It helps you find the most similar text chunks from your documents when someone asks a question.\n",
    "* Your LLM (HuggingFacePipeline)------------------------------------> Wraps a Hugging Face model as an LLM.\n",
    "* And your question-answering logic (RetrievalQA)-------------------> Connects retrieval and LLM to answer questions based on documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37142aa0-cf16-4b46-a487-3f8d5289368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer:\n",
      "A standard is a published technical document that represents a stakeholder consensus on how a material, product or assembly is to be designed, manufactured, tested or installed so that a construction\n",
      "\n",
      " Source Documents:\n",
      "\n",
      "--- Source 1 ---\n",
      "There is a long-standing relationship between building codes and standards that address design,\n",
      "installation, testing and materials related to building construction. Building regulation cannot be\n",
      "effectively carried out without such standards. The critical role of standards in the building\n",
      "\n",
      "--- Source 2 ---\n",
      "Standards and their relationship to the code\n",
      "A standard is a published technical document that represents a stakeholder consensus on how a\n",
      "material, product or assembly is to be designed, manufactured, tested or installed so that a\n",
      "\n",
      "--- Source 3 ---\n",
      "construction. When codes are adopted by units of government (i.e., towns, cities, counties,\n",
      "states, or other agencies or jurisdictions with regulatory authority), they provide the legal\n",
      "framework for regulations intended to promote public health, safety and welfare in the built\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the document. A text loader is used to load the document as raw text into memory, so it can be processed further.\n",
    "\n",
    "loader = TextLoader(\"llm_notes.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 3: Split the document. Each chunk has 300 characters, and 50 characters overlap with the next chunk for context continuity.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents[:5])  # limit size to reduce memory\n",
    "\n",
    "# Step 4: Embed using Hugging Face sentence transformer\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Step 5: Load flan-t5-small on CPU (safest config). Hugging Face’s pipeline is wrapped into a LangChain-compatible llm\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",\n",
    "    max_length=256,\n",
    "    device=device,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "\n",
    "# Step 6: Build the Retrieval QA chain. First retrieves top 3 relevant text chunks from FAISS, then passes them to the LLM to answer your query.\n",
    "#It enhances the LLM's ability to answer questions by grounding it in specific documents.\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Step 7: Ask a question\n",
    "#query = \"What is tokenization in LLMs and why is it important?\"\n",
    "query = \"What is the long-standing relationship between building codes and standards?\"\n",
    "result = qa_chain(query)\n",
    "\n",
    "# Step 8: Show results\n",
    "\n",
    "print(\"\\n Answer:\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "print(\"\\n Source Documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\n--- Source {i} ---\")\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf1942",
   "metadata": {},
   "source": [
    "# Big Picture Flow\n",
    "\n",
    "1- Load your notes.\n",
    "\n",
    "2- Split into chunks.\n",
    "\n",
    "3- Embed chunks into vectors.\n",
    "\n",
    "4- Store vectors in FAISS (vector database).\n",
    "\n",
    "5- Load an LLM (Flan-T5).\n",
    "\n",
    "6- Build a chain: query → retrieve top chunks → pass to LLM → get grounded answer.\n",
    "\n",
    "7- Print both the answer and the sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde3dd1",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ce315",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
    "! pip install langchain\n",
    "! pip install langchain-core\n",
    "! pip install langchain-openai\n",
    "! pip install langchain-community\n",
    "! pip install langchain-experimental\n",
    "! pip install langgraph\n",
    "! pip install \"langserve[all]\"\n",
    "! pip install langchain-cli\n",
    "! pip install langsmith\n",
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d37ff51",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mLANGSMITH_TRACING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mLANGSMITH_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mgetpass\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/py313/lib/python3.13/site-packages/ipykernel/kernelbase.py:1258\u001b[39m, in \u001b[36mKernel.getpass\u001b[39m\u001b[34m(self, prompt, stream)\u001b[39m\n\u001b[32m   1251\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m   1253\u001b[39m     warnings.warn(\n\u001b[32m   1254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `stream` parameter of `getpass.getpass` will have no effect when using ipykernel\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1255\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   1256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1257\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1260\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/py313/lib/python3.13/site-packages/ipykernel/kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c63923",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cfa1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2684c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713870c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://example.com/page1\",\n",
    "    \"https://example.com/page2\",\n",
    "]\n",
    "loader = WebBaseLoader(\n",
    "    web_paths= urls,\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,    # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    "    )\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
