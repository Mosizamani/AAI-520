{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8b29ed-9096-4db1-9be3-826a5daad55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai langchain faiss-cpu sentence-transformers\n",
    "#! pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4de108-400d-4667-bf05-a209082da613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Required Libraries for LLM + Document Retrieval Workflow\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a637f-97ac-43df-b5ba-2aa19c56078b",
   "metadata": {},
   "source": [
    "LangChain is a framework for building applications with language models (LLMs). \n",
    "- It simplifies the process of integrating LLMs with external data sources (like documents, databases, or APIs) \n",
    "- It allows you to create intelligent apps such as chatbots, retrieval-augmented generation (RAG) systems, and more.\n",
    "\n",
    "LangChain acts as the \"glue\" that connects:\n",
    "\n",
    "* Your documents (TextLoader)---------------------------------------> Loads plain text documents into LangChain.\n",
    "* Your text processing (TextSplitter)-------------------------------> Splits long text into manageable chunks.\n",
    "* Your embeddings (HuggingFaceEmbeddings)---------------------------> Generates embeddings for document chunks.\n",
    "* Your retrieval system (FAISS)-------------------------------------> Stores and retrieves embeddings efficiently using similarity    search.It helps you find the most similar text chunks from your documents when someone asks a question.\n",
    "* Your LLM (HuggingFacePipeline)------------------------------------> Wraps a Hugging Face model as an LLM.\n",
    "* And your question-answering logic (RetrievalQA)-------------------> Connects retrieval and LLM to answer questions based on documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37142aa0-cf16-4b46-a487-3f8d5289368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer:\n",
      "A standard is a published technical document that represents a stakeholder consensus on how a material, product or assembly is to be designed, manufactured, tested or installed so that a construction\n",
      "\n",
      " Source Documents:\n",
      "\n",
      "--- Source 1 ---\n",
      "There is a long-standing relationship between building codes and standards that address design,\n",
      "installation, testing and materials related to building construction. Building regulation cannot be\n",
      "effectively carried out without such standards. The critical role of standards in the building\n",
      "\n",
      "--- Source 2 ---\n",
      "Standards and their relationship to the code\n",
      "A standard is a published technical document that represents a stakeholder consensus on how a\n",
      "material, product or assembly is to be designed, manufactured, tested or installed so that a\n",
      "\n",
      "--- Source 3 ---\n",
      "construction. When codes are adopted by units of government (i.e., towns, cities, counties,\n",
      "states, or other agencies or jurisdictions with regulatory authority), they provide the legal\n",
      "framework for regulations intended to promote public health, safety and welfare in the built\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the document. A text loader is used to load the document as raw text into memory, so it can be processed further.\n",
    "\n",
    "loader = TextLoader(\"llm_notes.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 3: Split the document. Each chunk has 300 characters, and 50 characters overlap with the next chunk for context continuity.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents[:5])  # limit size to reduce memory\n",
    "\n",
    "# Step 4: Embed using Hugging Face sentence transformer\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Step 5: Load flan-t5-small on CPU (safest config). Hugging Face’s pipeline is wrapped into a LangChain-compatible llm\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",\n",
    "    max_length=256,\n",
    "    device=device,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "\n",
    "# Step 6: Build the Retrieval QA chain. First retrieves top 3 relevant text chunks from FAISS, then passes them to the LLM to answer your query.\n",
    "#It enhances the LLM's ability to answer questions by grounding it in specific documents.\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Step 7: Ask a question\n",
    "#query = \"What is tokenization in LLMs and why is it important?\"\n",
    "query = \"What is the long-standing relationship between building codes and standards?\"\n",
    "result = qa_chain(query)\n",
    "\n",
    "# Step 8: Show results\n",
    "\n",
    "print(\"\\n Answer:\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "print(\"\\n Source Documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\n--- Source {i} ---\")\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf1942",
   "metadata": {},
   "source": [
    "# Big Picture Flow\n",
    "\n",
    "1- Load your notes.\n",
    "\n",
    "2- Split into chunks.\n",
    "\n",
    "3- Embed chunks into vectors.\n",
    "\n",
    "4- Store vectors in FAISS (vector database).\n",
    "\n",
    "5- Load an LLM (Flan-T5).\n",
    "\n",
    "6- Build a chain: query → retrieve top chunks → pass to LLM → get grounded answer.\n",
    "\n",
    "7- Print both the answer and the sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
